---
title: "Corona Model"
author: "Abdelrahman Hammad"
date: "3/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Final Project : Part 2

Here we are loading in the data and necessary libraries. In addition, we are eliminating approximately 10% of data, Start date is 2020-03-12 and the end date is 2020-04-23, cutoff for 90% of total data is 2020-04-19.

```{r formatting and part a, message = F, echo = F}
#these libraries are necessary

#install.packages(forecast)
#install.packages("lubridate")
#install.packages("ggplot2")


library(readxl)
library(dplyr)
library(httr)
library(tseries)
library(astsa)
library(forecast)


#create the URL where the dataset is stored with automatic updates every day
url <- paste("https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide-",format(Sys.time(), "%Y-%m-%d"), ".xlsx", sep = "")

#download the dataset from the website to a local temporary file
GET(url, authenticate(":", ":", type="ntlm"), write_disk(tf <- tempfile(fileext = ".xlsx")))

#read the Dataset sheet into “R”
file <- read_excel(tf)

# make sure the date is formatted properly
file$dateRep <- as.POSIXct(file$dateRep, format = "%Y-%m-%d", tz = "America/New_York")

# subset data - eliminate approximately 10% of data
subsetfile <- subset(file, dateRep >= as.POSIXct('2020-03-12') & dateRep 
            <= as.POSIXct('2020-04-19'))

```

## Description of the data including a plot

```{r Plotting data}
US <- filter(subsetfile, geoId == "US")

US_data  <- data.frame(Cases = US$cases)

US_data  <- US_data[order(US_data$Cases),]


US_series <- ts(US_data)

plot(US_series)
```

	 The chosen dataset contains the amount of new cases of COVID-19 diagnosed daily across all infected countries. This project will specifically focus on COVID-19 cases reported per day in the United States since January 2, 2020. It was obtained from the European Union CDC equivalent ( European Union Center for Disease Control and Prevention ) and is updated daily with the amount of new cases. This growing data set is mitigated by only keeping track of a two month (60 days) span during the peak of the outbreak in the United States.

```{r part b}

acf2(US_series)

```

We must first observe a second wave before we can consider seasonality 
calling the acf function we can observe that the model is an AR(1) model 

## Transformations and/or differencing to achieve stationarity

```{r transformation/differencing}

#Differencing of order 1

US_series.dff1 = diff(US_series)
plot(US_series.dff1)

#Differencing of order 2

US_series.dff2 = diff(US_series.dff1)
plot(US_series.dff2)

###-------------------------------------------------

# checking for autocorrelation
Box.test(US_series.dff1, type = "Ljung-Box")
# checking stationarity of second ln difference 
adf.test(US_series.dff1, alternative = "stationary")

# checking for autocorrelation
Box.test(US_series.dff2, type = "Ljung-Box")
# checking stationarity of second ln difference 
adf.test(US_series.dff2, alternative = "stationary")


```

	From the plot of the data, there is an obvious increasing exponential trend in the number of cases over time. In order to make the data stationary, the data went through a cube root transformation and second differencing. This was proven by using the Augmented Dickey-Fuller Test which provided a significant p-value. 
	
## Estimated SARIMA model

```{r model selection}

acf2(US_series.dff2)

#Utilizing auto_arima

auto.arima(US_series.dff2, ic = 'aic', stationary = T, seasonal = F)
auto.arima(US_series.dff2, ic = 'bic', stationary = T, seasonal = F)
auto.arima(US_series.dff2, ic = 'aicc', stationary = T, seasonal = F)

# fitting several models

MA1_diff_2 = sarima(US_series.dff2, p=0, d=0, q=1)
MA1_diff_2

AR1_diff_2 = sarima(US_series.dff2, p=1, d=0, q=0)
AR1_diff_2

```

## Model Selection:

  From the ACF and PACF of the transformed data, there were several model options to explore ( MA(1), AR(1), AR(4) ). The best model for the data is the AR(4) model because it had the lowest variance, lowest AIC, and lowest BIC. 

## Residual Analysis:

	With the AR(4) model, most of the residual assumptions are satisfied. For example, the assumptions of normality, correlation, and independence of the residuals have been satisfied. However, the standardized residuals are not uniformly distributed around the mean. 


## Forecast/Predict the withheld data

```{r forecast}

sarima.for(US_series, n.ahead=4, p=0, d=2, q=1, P=0, D=0, Q=0, S=0)

```


